---
title: 主成分分析
tags:
  - 主成分分析
categories: 人工智能
mathjax: true
abbrlink: '7276e58'
date: 2025-12-13 21:06:54
---

假设有一组二维数据点，对应的数据矩阵为 $X$

- 数据矩阵中心化

质心的位置为

$$
\mu
=\dfrac{X^{\top}}{n}
$$

将数据中心化（去均值）

$$
X_c = X - \mu^{\top}
$$

- 协方差矩阵

$$
\Sigma = \dfrac{X_c^{\top}X_c}{n-1}
=\begin{bmatrix}
\sigma_1 & \mathrm{cov}(x_1, x_2) & \cdots\\
\mathrm{cov}(x_2, x_1) & \sigma_2 & \cdots\\
\vdots & \vdots & \ddots
\end{bmatrix}
$$

> 如果不同特征的标准差差异过大，需要先对数据标准化，不再依赖原本的度量单位。
>
> $$
> Z = X_cD^{-1} = (X - \mu^{\top})D^{-1}, D = \mathrm{diag}(\sigma_1, \sigma_2, \cdots)
> $$
>
> 经过标准化处理后，数据均值变为 $0$，标准差变为 $1$.

协方差矩阵的主对角线为方差 $\sigma_1, \sigma_2,\cdots$，非对角线元素为协方差 $\mathrm{cov}(x_1, x_2), \mathrm{cov}(x_2, x_1),\cdots$

线性相关系数为

$$
\rho_{1,2} = \dfrac{\mathrm{cov}(x_1, x_2)}{\sigma_1\sigma_2}
$$

线性相关性系数矩阵为

$$
\Rho = \begin{bmatrix}
1 & \rho_{1,2} &\cdots\\
\rho_{1,2} & 1 & \cdots\\
\vdots & \vdots & \ddots
\end{bmatrix}
$$

每个元素表示两个特征之间的相关性 $[-1,1]$，对角线上恒为 1，表示和自身完全正相关。

$$
\Rho = \Sigma_{Z} = \dfrac{Z^{\top}Z}{n-1}\\
$$

---

从正交投影的角度看，$Z$ 在 $e_1$ 方向的投影 $z_1$ 的方差为

$$
\mathrm{var}(z_1) = \dfrac{z_1^{\top}z_1}{n-1} = \dfrac{(Ze_1)^{\top}(Z e_1)}{n-1}
= \dfrac{e_1^{\top}Z^{\top}Z e_1}{n-1} = e_1^{\top}\Rho e_1 = 1
$$

$z_1, z_2$ 的协方差为

$$
\mathrm{cov}(z_1, z_2) = \mathrm{cov}(z_2, z_1) = \dfrac{z_2^{\top}z_1}{n-1}
= \dfrac{(Ze_2)^{\top}(Z e_1)}{n-1}
= \dfrac{e_2^{\top}Z^{\top}Z e_1}{n-1} = e_2^{\top}\Rho e_1 =\rho_{1,2}
$$

---

将线性相关性系数矩阵进行特征分解，得到

$$
\Rho = V\Lambda V^{\top}
$$

$Z$ 在特征向量 $v_1$ 方向的投影为

$$
y = Zv
$$

则 $y_1$ 的方差为

$$
\mathrm{var}(y) = \dfrac{y^{\top}y}{n-1} = \dfrac{(Zv)^{\top}(Zv)}{n-1}
= \dfrac{v^{\top}Z^{\top}Zv}{n-1} = v^{\top}\Rho v\\
$$

$Z$ 朝 $V$ 正交投影得到 $Y$，

$$
\Sigma_Y = \dfrac{Y^{\top}Y}{n-1} = \dfrac{V^{\top}Z^{\top}ZV}{n-1} = V^{\top}\Rho V = \Lambda\\
$$

特征值越大，表示该方向上数据的方差越大，信息越丰富（椭圆/球越长）。

> 把数据看作一个旋转的椭圆，主成分分析就是找到合适的方向将椭圆摆正。

- 近似还原

如果仅用第一主元信息还原 $Z$，对应的运算为

$$
\hat{Z} = Z_1 = y_1 v_1^{\top} = Z(v_1 v_1^{\top})
$$

误差为

$$
Z - Z_1 = Z(I - v_1 v_1^{\top})
$$

然后用 $Z_1$ 近似还原 $X$，进一步缩放和平移

$$
\hat{X} = Z_1D + \mu^{\top} = Z(v_1 v_1^{\top})D + \mu^{\top}
$$

类似地，用前两个主元信息还原 $X$，对应的运算为

$$
\hat{X} = (Z_1 + Z_2)D + \mu^{\top} = Z(v_1 v_1^{\top} + v_2 v_2^{\top})D + \mu^{\top}
$$

## 参考资料

[Visualize-ML/Linear-Algebra-Made-Easy---Learn-with-Python-and-Visualization](https://github.com/Visualize-ML/Linear-Algebra-Made-Easy---Learn-with-Python-and-Visualization)
