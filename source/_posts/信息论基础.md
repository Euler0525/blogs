---
title: 信息论基础
tags:
  - 熵
  - 相对熵
  - Kullback-Leibler Divergence
  - 数据处理不等式
  - 充分统计量
  - 费诺不等式
  - 渐进均分性
  - 信源编码
  - 马尔科夫链
categories: 基础知识
mathjax: true
abbrlink: b37d7559
date: 2024-04-23 14:13:40
---

# 信息论基础

## 熵和互信息的关系

<img src="https://cdn.jsdelivr.net/gh/Euler0525/tube@master/ct/entropy_mutual_info.webp" width="60%" height="60%" />

## 相对熵(KL距离)

相对熵衡量的是同一事件空间中两个概率分布的差异，*不是距离*。

> 在同一事件空间中，概率分布$P(X)$对应的每个事件如果用$Q(x)$编码，平均每个基本事件编码长度增加了多少比特。

用$D(p||q)$表示$KL$距离，计算公式为
$$
D(p||q) = \sum_{x\in X}P(x)\log_2{\dfrac{P(x)}{Q(x)}}
$$
两分布相似度越高，相对熵越小。

- 非负性（由吉布斯不等式可证）


> 若$\displaystyle\sum_{i=1}^{n}p_i=\displaystyle\sum_{i=1}^{n}q_i=1$，则$-\displaystyle\sum_{i=1}^np_i\log{p_i}\leq -\displaystyle\sum_{i=1}^np_i\log{q_i}$，当且仅当$\forall i,p_i=q_i$​时取等


- 不满足对称性（交叉熵$-$相对熵）
- 不满足三角不等式

## 数据处理不等式

如果$Z$的条件分布仅依赖$Y$的分布，与$X$条件独立，随机变量$X,Y,Z$构成马尔科夫链，满足$I(X;Z|Y)=0$

记为$X\rightarrow Y\rightarrow Z$，则$I(X;Y)\geq I(X;Z)$​。如果$Z=g(Y)$，则$I(X;Y)\geq I(X;g(Y))$.

> 数据越传损失的越多；数据$Y$的函数不会增加关于$X$的信息量~

如果$X\rightarrow Y \rightarrow Z$，则$I(X;Y)\geq I(X;Y|Z)$​。

> 通过观察$Z$，$X$与$Y$的依赖会降低~

## Fano不等式

*(+﹏+)~晕，待续……*

## 渐进均分性

### 收敛

- **收敛**：图像逐渐趋近于某条横线

$\forall \varepsilon > 0, \exist N\in N^+$，当$n>N$时，满足$|a_n-a|<\varepsilon$，即$\displaystyle\lim_{n\to \infty}{|a_n-a|}=0$，记作$a_n\to a$

- **几乎确定收敛**：图像逐渐趋近于某条横线，但是有几个点各色

$\forall \varepsilon > 0, \exist N\in N^+$，当$n>N$时，满足$P(|a_n-a|<\varepsilon)=1$，即$P(\displaystyle\lim_{n\to \infty}{|a_n-a|}\leq \varepsilon)=1$

- **依概率收敛**：

$\delta>0, \exist N\in N^+$，当$n>N$时，满足$|P(|a_n-a|<\varepsilon)-1|\leq \delta$，即$\displaystyle\lim_{n\to \infty}P({|a_n-a|}\leq \varepsilon)=1$，记作$a_n\stackrel{P}\to a$​.

### 大数定理

- **强大数定理**：$\mu_n$一直向$\mu$靠近

$$
\forall \varepsilon>0,P(\displaystyle\lim_{n\to \infty}{|\mu_n-\mu|<\varepsilon})=1
$$

- **弱大数定理**：$\mu_n$向$\mu$靠近的可能性越来越大

$$
\forall \varepsilon>0,\displaystyle\lim_{n\to \infty}P({|\mu_n-\mu|<\varepsilon})=1
$$

### 渐进均分定理(AEP)

当序列足够长，其中一部分序列就会显示出某种固定的性质，即各个符号出现的频率接近于概率，而这些序列的的概率则趋近于相等，且它们的和非常接近于$1$。这些序列就是**典型序列**。其余不具备这种性质的序列，称之为**非典型序列**，这些非典型序列的出现概率之和接近于零。序列的长度越长，典型序列的总概率就越接近于$1$​，它的各个序列的出现概率越趋近于相等，这种现象称为**渐进均分性**。

数学定义为：若$X_1,X_2,\cdots,X_n$满足$i.i.d$
$$
\frac{1}{n}\log{\dfrac{1}{P(X_1,X_2\cdots,X_n)}} \stackrel{P}\to H(X)
$$

### 典型集

若$X_1,X_2,\cdots,X_n$是概率密度函数为$p(x)$的$i.i.d$，满足
$$
\begin{aligned}
&|-\frac{1}{n}\log{P(X_1,X_2\cdots,X_n)} - H(X)| \leq \varepsilon\\
&\iff 2^{-n(H(X)+\varepsilon)}\leq P(X_1,X_2,\cdots,X_n)\leq 2^{-n(H(X)-\varepsilon)}
\end{aligned}
$$
称该序列为典型集，记作$A_{\varepsilon}^{(n)}$.

**性质**

- 若$x_1,x_2,\cdots,x_n \in A_{\varepsilon}^{(n)}$，则$H(X)-\varepsilon\leq-\frac{1}{n}\log{P(X_1,X_2\cdots,X_n)}\leq H(X)+\varepsilon$​
- $|A_{\varepsilon}^{(n)}|\leq 2^{n(H(X)+\varepsilon)}$，<u>典型集中的元素个数</u>
- 当$n\to \infty$时，$P(A_{\varepsilon}^{(n)})>1-\varepsilon$
- 当$n\to \infty$，$|A_{\varepsilon}^{(n)}|\geq(1-\varepsilon)2^{n(H(X)-\varepsilon)}$

### 数据压缩

数据压缩时把一个完整的概率空间缩小到典型集，而典型集相对整个空间来说时非常小的，且高概率出现，这就起到了压缩的作用。

> ***典型集的个数很少***
>
> 如果全空间有$n$个元素，根据性质2它大概有$2^{nH}$个元素。
>
> $\displaystyle\lim_{n\to\infty}\dfrac{2^{nH(X)}}{2^n}=\displaystyle\lim_{n\to \infty}2^{n(H(X)-1)}=0,(H(X)\leq1)$​
>
> 这说明典型集的个数相对于全空间来说时很小的，但他确实高频率出现的。

#### 数据压缩

1. 将集合元素按照某种顺序排列；
2. 指定下标可表示$A_{\epsilon}^{(n)}$中的每个序列；
3. $|A_{\varepsilon}^{(n)}|\leq 2^{n(H+\varepsilon)}$需要$n(H+\varepsilon)+1$比特；
4. 编码加$0$，则编码$A_{\varepsilon}^{(n)}$需要$n(H+\varepsilon)+2$​比特；
5. 同理，对于非典型集$A_{\varepsilon}^{(n)}$需要$n\log{|\mathcal{X}|}+1$比特，编码加$1$需要$n\log{|\mathcal{X}|}+2$比特；
6. 获得一个$\mathcal{X}^n$一个编码方案；

> 其中非典型集的比特数是用全空间序列计算的，因为典型集个数很少，可以用全空间元素个数代替非典型集元素个数，误差可以忽略。

#### 编码方案

设$l(x^n)$表示相应于$x^n$的码字长度，若$n$充分大，使得$P(A_{\varepsilon}^{(n)})\geq 1-\varepsilon$，于是码字长度的期望满足
$$
E(\dfrac{1}{n}l(X^n)) \leq H(X)+\varepsilon
$$
因此，从平均意义上，用$nH(x)$比特可以表示序列$X^n$

## 熵率

前文中的渐进均分性(AEP)表示平均意义下使用$nH(X)$比特足够描述$n$个$i.i.d$​的随机变量，但是随机变量不独立，比如平稳随机过程时……

### 马尔科夫链

用马尔科夫链可以减少研究随机过程问题的维度。

马尔科夫过程的每一步结果最多只与上一步有关，与其它步骤无关，数学表示为
$$
P(X_1,X_2,\cdots,X_n) = P(X_1)P(X_2|X_1)P(X_3|X_2)\cdots P(X_n|X_{n-1})
$$

> 马尔科夫链不是一个真正与历史无关的过程，通过链式法则，历史的信息可以传递到现在。

## 信道容量

香农信道容量公式
$$
C = B \log_2{1 + \dfrac{S}{N}}
$$
对于典型干扰环境$\dfrac{S}{N}<<1$，则有
$$
\dfrac{C}{B} \approx 1.44 \dfrac{S}{N}
$$

- 在信道中当传输系统的$\dfrac{S}{N}$下降时，可以用增加系统传输带宽$B$的办法来保持信道容量$C$不变；
- 在高斯白噪声干扰情况下，在平均功率受限的信道上，实现有效和可靠通信的最佳信号是具有白噪声统计特性的信号。

## 参考资料

1. [https://www.jiqizhixin.com/articles/0224](https://www.jiqizhixin.com/articles/0224)
1. [https://zhuanlan.zhihu.com/p/149188816](https://zhuanlan.zhihu.com/p/149188816)
