---
title: 人工智能基础
tags:
  - 线性回归
  - 最大似然估计
  - 最大后验估计
categories: 人工智能
mathjax: true
abbrlink: a1bded03
date: 2025-10-24 16:44:58
---

# 人工智能基础

## 线性回归

设模型的预测值为 $f(x_k) = w^{\top}x_k$，观测值为 $ y_k $，则噪声 $ \varepsilon_k = y_k - f(x_k) $。噪声服从参数为 $(0, \sigma^2)$ 的正态分布，即 $\varepsilon_k\sim N(0, \sigma^2)$，其概率密度函数为
$$
p(\varepsilon_k) = \dfrac{1}{\sqrt{2\pi}\sigma}\exp{(-\frac{\varepsilon_k^2}{2\sigma^2})}
$$
**单个样本 $ (x_k, y_k) $ 出现的概率等价于噪声取值为 $\varepsilon_k=y_k-f(x_k)$ 的概率**，将 $\varepsilon_k=y_k-w^{\top}x_k$ 带入噪声的概率密度函数得到单个样本的概率密度
$$
p(y_k|x_k; w) = \dfrac{1}{\sqrt{2\pi}\sigma}\exp{\left(-\frac{(y_k-w^{\top}x_k)^2}{2\sigma^2}\right)}
$$
其中 $p(y_k|x_k;w)$ 表示给定 $ x_k $ 和参数 $ w $ 的条件下，观测到 $ y_k $ 的概率密度，核心是模型参数 $ w $ 的函数。

若有 $ N $ 个样本 $\{(x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N)\}$，各个样本之间相互独立，则所有样本同时出现的概率为
$$
\begin{aligned}
L(w) &= \prod_{k = 1}^N p(y_k|x_k; w) = \prod_{k = 1}^N \dfrac{1}{\sqrt{2\pi}\sigma}\exp{\left(-\frac{(y_k-w^{\top}x_k)^2}{2\sigma^2}\right)}\\
\ln L(w) &= -\dfrac{N}{2}\ln(2\pi) - N\ln \sigma -\frac{1}{2\sigma^2}\sum_{k = 1}^N{(y_k-w^{\top}x_k)^2}
\end{aligned}
$$
于是，最大化似然函数 $ L(w) $ 等价于最小化 $\displaystyle\sum_{k=1}^N{(y_k-w^{\top}x_k)^2}$，即最小二乘法的损失函数，同时也证明了在噪声满足正态分布的条件下，最小二乘与最大似然等价。

对于单变量的线性回归，$y=w_1x + w_0$，带入到均方误差的表达式中对 $ w_1 $ 和 $w_0$ 求偏导， 极值点即为线性回归的最优解
$$
\begin{aligned}
w_1 &= \dfrac{\displaystyle\sum_{k = 1}^{N}y_k(x_k - \displaystyle\frac{1}{N}\sum_{k = 1}^N x_k)}{\displaystyle\sum_{k = 1}^N x_k^2 - \frac{1}{N}(\displaystyle\sum_{k = 1}^N x_k)^2}\\
w_0 &= \dfrac{1}{N} \sum_{k = 1}^N (y_k - w_1 x_k)\\
\end{aligned}
$$
对于多变量的线性回归，$\displaystyle\sum_{k=1}^N{(y_k-w^{\top}x_k)^2} = \displaystyle\sum_{k=1}^N{(y_k-w^{\top}x_k)^{\top}(y_k-w^{\top}x_k)} = (\mathbf{y} - \mathbf{X}\mathbf{w})^{\top}(\mathbf{y} - \mathbf{X}\mathbf{w})$。

定义损失函数为
$$
\begin{aligned}
L &= (\mathbf{y} - \mathbf{X}\mathbf{w})^{\top}(\mathbf{y} - \mathbf{X}\mathbf{w})\\
&= \mathbf{y}^{\top}\mathbf{y} - 2\mathbf{w}^{\top}\mathbf{X}^{\top}\mathbf{y} + \mathbf{w}^T(\mathbf{X}^{\top}\mathbf{X})\mathbf{w}\\
\end{aligned}
$$
令 $\dfrac{\partial{L}}{\partial \mathbf{w}} = -  2\mathbf{X}^{\top}\mathbf{y} + 2\mathbf{X}^{\top}\mathbf{X}\mathbf{w} = 0$，在 $\mathbf{X}^{\top}\mathbf{X}$ 的逆矩阵存在的前提下，得到参数的最优解为
$$
\mathbf{w} = (\mathbf{X}^{\top}\mathbf{X})^{-1}\mathbf{X}\mathbf{y}
$$
*持续更新中……*

## 参考资料

[人工智能基础课](https://time.geekbang.org/column/intro/100003101?tab=catalog)
